{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfb6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import albumentations\n",
    "import albumentations.pytorch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0ecdaa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-18 07:31:14.174949: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-18 07:31:14.174978: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('runs/')\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294e6c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(111)\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.manual_seed_all(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7957a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, transform = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_dir = \"/home/temp_1/kangsanha/malocclusion/data\"\n",
    "        self.filenames = os.listdir(self.img_dir)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = MakeLabel(self.filenames[index])\n",
    "        img_path = os.path.join(self.img_dir, self.filenames[index])\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # BGR -> RGB\n",
    "        # albumentations 라이브러리를 사용할 것인데, PIL을 쓰면 변환이 안된다하여 cv2로 변환해준다.\n",
    "        # img = np.asarray_chkfinite(Image.open(img_path))\n",
    "        \n",
    "        if img.dtype == np.uint8:\n",
    "            img = img / 255.0\n",
    "        \n",
    "        #img = ToTensor(img)\n",
    "        #print(type(img))\n",
    "        #print(np.shape(img))\n",
    "        #img = img.transpose((2,0,1)).astype(np.float32)\n",
    "        if self.transform:\n",
    "            aug = self.transform(image=img)\n",
    "            img = aug['image']\n",
    "        #img = img.values()\n",
    "        \n",
    "        #  print(np.shape(img)) # 검사코드\n",
    "        # input = {'data' : img, 'label' : label}\n",
    "        \n",
    "        return img, label\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.filenames)\n",
    "\n",
    "    \n",
    "def MakeLabel(filenames):\n",
    "    input_img = filenames\n",
    "    if \"O\" in input_img:\n",
    "        label = 1#label = F.one_hot(torch.tensor(0), 2)\n",
    "    else : label = 0#label = F.one_hot(torch.tensor(1), 2)\n",
    "\n",
    "    return label\n",
    "\n",
    "def ToTensor(image):\n",
    "    image = image.transpose((2,0,1)).astype(np.float32)\n",
    "    image = torch.from_numpy(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dffff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# albumentations train\n",
    "\n",
    "albumentations_train = albumentations.Compose([\n",
    "    albumentations.Resize(height=256, width=256),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.HorizontalFlip(p=0.8),\n",
    "        albumentations.RandomRotate90(p=0.8),\n",
    "        albumentations.VerticalFlip(p=0.8)\n",
    "    ], p=1),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.MotionBlur(p=0.75),\n",
    "        albumentations.OpticalDistortion(p=0.75),\n",
    "        albumentations.GaussNoise(p=0.75)\n",
    "    ], p=1),\n",
    "    albumentations.pytorch.ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e871608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = Dataset(transform = albumentations_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c42ff3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000, 1.0000, 0.0000,  ..., 1.0000, 1.0000, 0.0000],\n",
       "          [0.0000, 1.0000, 1.0000,  ..., 0.0000, 0.7128, 0.0000],\n",
       "          [1.0000, 1.0000, 0.0000,  ..., 1.0000, 0.4569, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "          [1.0000, 1.0000, 0.5446,  ..., 0.0000, 1.0000, 1.0000],\n",
       "          [0.0022, 0.0000, 1.0000,  ..., 1.0000, 1.0000, 0.0000]],\n",
       " \n",
       "         [[1.0000, 1.0000, 0.0000,  ..., 1.0000, 1.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 1.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 1.0000,  ..., 1.0000, 0.0000, 1.0000]],\n",
       " \n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 0.0000, 0.6073,  ..., 0.0000, 1.0000, 0.0000],\n",
       "          [0.0000, 1.0000, 0.0000,  ..., 0.0000, 0.9495, 0.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.2309, 0.0000,  ..., 1.0000, 1.0000, 0.0000]]],\n",
       "        dtype=torch.float64),\n",
       " 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6e26182",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_datset = torch.utils.data.random_split(full_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f82a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_datset, batch_size=4, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1d10210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(ResNet50)\n",
    "\n",
    "import torchvision.models.resnet as resnet\n",
    "import torch.optim as optim\n",
    "\n",
    "conv1x1 = resnet.conv1x1\n",
    "Bottleneck = resnet.Bottleneck\n",
    "BasicBlock = resnet.BasicBlock\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=2, zero_init_residual=True):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 32\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False) # 마찬가지로 전부 사이즈 조정\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 32, layers[0], stride=1) # 3 반복\n",
    "        self.layer2 = self._make_layer(block, 64, layers[1], stride=2) # 4 반복\n",
    "        self.layer3 = self._make_layer(block, 128, layers[2], stride=2) # 6 반복\n",
    "        self.layer4 = self._make_layer(block, 256, layers[3], stride=2) # 3 반복\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1): # planes -> 입력되는 채널 수\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion: \n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input [32, 128, 128] -> [C ,H, W]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        #x.shape =[32, 64, 64]\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        #x.shape =[128, 64, 64]\n",
    "        x = self.layer2(x)\n",
    "        #x.shape =[256, 32, 32]\n",
    "        x = self.layer3(x)\n",
    "        #x.shape =[512, 16, 16]\n",
    "        x = self.layer4(x)\n",
    "        #x.shape =[1024, 8, 8]\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ed06ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = ResNet(resnet.Bottleneck, [3,4,6,3], 2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3d4e8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d53c2cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정의\n",
    "optimizer = torch.optim.Adam(resnet50.parameters(), lr=1e-5)\n",
    "loss_func = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f53855b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 128, 128]             864\n",
      "       BatchNorm2d-2         [-1, 32, 128, 128]              64\n",
      "              ReLU-3         [-1, 32, 128, 128]               0\n",
      "         MaxPool2d-4           [-1, 32, 64, 64]               0\n",
      "            Conv2d-5           [-1, 32, 64, 64]           1,024\n",
      "       BatchNorm2d-6           [-1, 32, 64, 64]              64\n",
      "              ReLU-7           [-1, 32, 64, 64]               0\n",
      "            Conv2d-8           [-1, 32, 64, 64]           9,216\n",
      "       BatchNorm2d-9           [-1, 32, 64, 64]              64\n",
      "             ReLU-10           [-1, 32, 64, 64]               0\n",
      "           Conv2d-11          [-1, 128, 64, 64]           4,096\n",
      "      BatchNorm2d-12          [-1, 128, 64, 64]             256\n",
      "           Conv2d-13          [-1, 128, 64, 64]           4,096\n",
      "      BatchNorm2d-14          [-1, 128, 64, 64]             256\n",
      "             ReLU-15          [-1, 128, 64, 64]               0\n",
      "       Bottleneck-16          [-1, 128, 64, 64]               0\n",
      "           Conv2d-17           [-1, 32, 64, 64]           4,096\n",
      "      BatchNorm2d-18           [-1, 32, 64, 64]              64\n",
      "             ReLU-19           [-1, 32, 64, 64]               0\n",
      "           Conv2d-20           [-1, 32, 64, 64]           9,216\n",
      "      BatchNorm2d-21           [-1, 32, 64, 64]              64\n",
      "             ReLU-22           [-1, 32, 64, 64]               0\n",
      "           Conv2d-23          [-1, 128, 64, 64]           4,096\n",
      "      BatchNorm2d-24          [-1, 128, 64, 64]             256\n",
      "             ReLU-25          [-1, 128, 64, 64]               0\n",
      "       Bottleneck-26          [-1, 128, 64, 64]               0\n",
      "           Conv2d-27           [-1, 32, 64, 64]           4,096\n",
      "      BatchNorm2d-28           [-1, 32, 64, 64]              64\n",
      "             ReLU-29           [-1, 32, 64, 64]               0\n",
      "           Conv2d-30           [-1, 32, 64, 64]           9,216\n",
      "      BatchNorm2d-31           [-1, 32, 64, 64]              64\n",
      "             ReLU-32           [-1, 32, 64, 64]               0\n",
      "           Conv2d-33          [-1, 128, 64, 64]           4,096\n",
      "      BatchNorm2d-34          [-1, 128, 64, 64]             256\n",
      "             ReLU-35          [-1, 128, 64, 64]               0\n",
      "       Bottleneck-36          [-1, 128, 64, 64]               0\n",
      "           Conv2d-37           [-1, 64, 64, 64]           8,192\n",
      "      BatchNorm2d-38           [-1, 64, 64, 64]             128\n",
      "             ReLU-39           [-1, 64, 64, 64]               0\n",
      "           Conv2d-40           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-41           [-1, 64, 32, 32]             128\n",
      "             ReLU-42           [-1, 64, 32, 32]               0\n",
      "           Conv2d-43          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-44          [-1, 256, 32, 32]             512\n",
      "           Conv2d-45          [-1, 256, 32, 32]          32,768\n",
      "      BatchNorm2d-46          [-1, 256, 32, 32]             512\n",
      "             ReLU-47          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-48          [-1, 256, 32, 32]               0\n",
      "           Conv2d-49           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-50           [-1, 64, 32, 32]             128\n",
      "             ReLU-51           [-1, 64, 32, 32]               0\n",
      "           Conv2d-52           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-53           [-1, 64, 32, 32]             128\n",
      "             ReLU-54           [-1, 64, 32, 32]               0\n",
      "           Conv2d-55          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-56          [-1, 256, 32, 32]             512\n",
      "             ReLU-57          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-58          [-1, 256, 32, 32]               0\n",
      "           Conv2d-59           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-60           [-1, 64, 32, 32]             128\n",
      "             ReLU-61           [-1, 64, 32, 32]               0\n",
      "           Conv2d-62           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-63           [-1, 64, 32, 32]             128\n",
      "             ReLU-64           [-1, 64, 32, 32]               0\n",
      "           Conv2d-65          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-66          [-1, 256, 32, 32]             512\n",
      "             ReLU-67          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-68          [-1, 256, 32, 32]               0\n",
      "           Conv2d-69           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-70           [-1, 64, 32, 32]             128\n",
      "             ReLU-71           [-1, 64, 32, 32]               0\n",
      "           Conv2d-72           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-73           [-1, 64, 32, 32]             128\n",
      "             ReLU-74           [-1, 64, 32, 32]               0\n",
      "           Conv2d-75          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-76          [-1, 256, 32, 32]             512\n",
      "             ReLU-77          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-78          [-1, 256, 32, 32]               0\n",
      "           Conv2d-79          [-1, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-80          [-1, 128, 32, 32]             256\n",
      "             ReLU-81          [-1, 128, 32, 32]               0\n",
      "           Conv2d-82          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-83          [-1, 128, 16, 16]             256\n",
      "             ReLU-84          [-1, 128, 16, 16]               0\n",
      "           Conv2d-85          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-86          [-1, 512, 16, 16]           1,024\n",
      "           Conv2d-87          [-1, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-88          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-89          [-1, 512, 16, 16]               0\n",
      "       Bottleneck-90          [-1, 512, 16, 16]               0\n",
      "           Conv2d-91          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-92          [-1, 128, 16, 16]             256\n",
      "             ReLU-93          [-1, 128, 16, 16]               0\n",
      "           Conv2d-94          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-95          [-1, 128, 16, 16]             256\n",
      "             ReLU-96          [-1, 128, 16, 16]               0\n",
      "           Conv2d-97          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-98          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-99          [-1, 512, 16, 16]               0\n",
      "      Bottleneck-100          [-1, 512, 16, 16]               0\n",
      "          Conv2d-101          [-1, 128, 16, 16]          65,536\n",
      "     BatchNorm2d-102          [-1, 128, 16, 16]             256\n",
      "            ReLU-103          [-1, 128, 16, 16]               0\n",
      "          Conv2d-104          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-105          [-1, 128, 16, 16]             256\n",
      "            ReLU-106          [-1, 128, 16, 16]               0\n",
      "          Conv2d-107          [-1, 512, 16, 16]          65,536\n",
      "     BatchNorm2d-108          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-109          [-1, 512, 16, 16]               0\n",
      "      Bottleneck-110          [-1, 512, 16, 16]               0\n",
      "          Conv2d-111          [-1, 128, 16, 16]          65,536\n",
      "     BatchNorm2d-112          [-1, 128, 16, 16]             256\n",
      "            ReLU-113          [-1, 128, 16, 16]               0\n",
      "          Conv2d-114          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-115          [-1, 128, 16, 16]             256\n",
      "            ReLU-116          [-1, 128, 16, 16]               0\n",
      "          Conv2d-117          [-1, 512, 16, 16]          65,536\n",
      "     BatchNorm2d-118          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-119          [-1, 512, 16, 16]               0\n",
      "      Bottleneck-120          [-1, 512, 16, 16]               0\n",
      "          Conv2d-121          [-1, 128, 16, 16]          65,536\n",
      "     BatchNorm2d-122          [-1, 128, 16, 16]             256\n",
      "            ReLU-123          [-1, 128, 16, 16]               0\n",
      "          Conv2d-124          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-125          [-1, 128, 16, 16]             256\n",
      "            ReLU-126          [-1, 128, 16, 16]               0\n",
      "          Conv2d-127          [-1, 512, 16, 16]          65,536\n",
      "     BatchNorm2d-128          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-129          [-1, 512, 16, 16]               0\n",
      "      Bottleneck-130          [-1, 512, 16, 16]               0\n",
      "          Conv2d-131          [-1, 128, 16, 16]          65,536\n",
      "     BatchNorm2d-132          [-1, 128, 16, 16]             256\n",
      "            ReLU-133          [-1, 128, 16, 16]               0\n",
      "          Conv2d-134          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-135          [-1, 128, 16, 16]             256\n",
      "            ReLU-136          [-1, 128, 16, 16]               0\n",
      "          Conv2d-137          [-1, 512, 16, 16]          65,536\n",
      "     BatchNorm2d-138          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-139          [-1, 512, 16, 16]               0\n",
      "      Bottleneck-140          [-1, 512, 16, 16]               0\n",
      "          Conv2d-141          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-142          [-1, 256, 16, 16]             512\n",
      "            ReLU-143          [-1, 256, 16, 16]               0\n",
      "          Conv2d-144            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-145            [-1, 256, 8, 8]             512\n",
      "            ReLU-146            [-1, 256, 8, 8]               0\n",
      "          Conv2d-147           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-148           [-1, 1024, 8, 8]           2,048\n",
      "          Conv2d-149           [-1, 1024, 8, 8]         524,288\n",
      "     BatchNorm2d-150           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-151           [-1, 1024, 8, 8]               0\n",
      "      Bottleneck-152           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-153            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-154            [-1, 256, 8, 8]             512\n",
      "            ReLU-155            [-1, 256, 8, 8]               0\n",
      "          Conv2d-156            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-157            [-1, 256, 8, 8]             512\n",
      "            ReLU-158            [-1, 256, 8, 8]               0\n",
      "          Conv2d-159           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-160           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-161           [-1, 1024, 8, 8]               0\n",
      "      Bottleneck-162           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-163            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-164            [-1, 256, 8, 8]             512\n",
      "            ReLU-165            [-1, 256, 8, 8]               0\n",
      "          Conv2d-166            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-167            [-1, 256, 8, 8]             512\n",
      "            ReLU-168            [-1, 256, 8, 8]               0\n",
      "          Conv2d-169           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-170           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-171           [-1, 1024, 8, 8]               0\n",
      "      Bottleneck-172           [-1, 1024, 8, 8]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 1024, 1, 1]               0\n",
      "          Linear-174                    [-1, 2]           2,050\n",
      "================================================================\n",
      "Total params: 5,890,850\n",
      "Trainable params: 5,890,850\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 187.13\n",
      "Params size (MB): 22.47\n",
      "Estimated Total Size (MB): 209.79\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(resnet50, input_size=(3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e4c67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c182822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "image, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6de94060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 그리드 만들기\n",
    "img_grid = torchvision.utils.make_grid(image)\n",
    "writer.add_image('inhovation_images', img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "410e00f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조 올리기\n",
    "writer.add_graph(resnet50, image.cuda().float())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "162bc973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전이 학습\n",
    "resnet50 = torchvision.models.resnet50(pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "927e3f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50.fc = nn.Linear(resnet50.fc.in_features, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8318c39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                    [-1, 2]           4,098\n",
      "================================================================\n",
      "Total params: 23,512,130\n",
      "Trainable params: 23,512,130\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.55\n",
      "Params size (MB): 89.69\n",
      "Estimated Total Size (MB): 376.82\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(resnet50, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e1f7a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2c33152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    학습된 신경망과 이미지 목록으로부터 예측 결과 및 확률을 생성합니다\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.cpu().numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5040d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    학습된 신경망과 배치로부터 가져온 이미지 / 라벨을 사용하여 matplotlib\n",
    "    Figure를 생성합니다. 이는 신경망의 예측 결과 / 확률과 함께 정답을 보여주며,\n",
    "    예측 결과가 맞았는지 여부에 따라 색을 다르게 표시합니다. \"images_to_probs\"\n",
    "    함수를 사용합니다.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # 배치에서 이미지를 가져와 예측 결과 / 정답과 함께 표시(plot)합니다\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb25fb9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0000 / 0100 | BATCH 0134 / 0135 | LOSS 0.6809\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0134 / 0135 | LOSS 0.6774\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0134 / 0135 | LOSS 0.6807\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0134 / 0135 | LOSS 0.6784\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0134 / 0135 | LOSS 0.6774\n",
      "TRAIN: EPOCH 0005 / 0100 | BATCH 0134 / 0135 | LOSS 0.6761\n",
      "TRAIN: EPOCH 0006 / 0100 | BATCH 0134 / 0135 | LOSS 0.6808\n",
      "TRAIN: EPOCH 0007 / 0100 | BATCH 0134 / 0135 | LOSS 0.6816\n",
      "TRAIN: EPOCH 0008 / 0100 | BATCH 0134 / 0135 | LOSS 0.6772\n",
      "TRAIN: EPOCH 0009 / 0100 | BATCH 0134 / 0135 | LOSS 0.6827\n",
      "TRAIN: EPOCH 0010 / 0100 | BATCH 0134 / 0135 | LOSS 0.6774\n",
      "TRAIN: EPOCH 0011 / 0100 | BATCH 0134 / 0135 | LOSS 0.6808\n",
      "TRAIN: EPOCH 0012 / 0100 | BATCH 0134 / 0135 | LOSS 0.6789\n",
      "TRAIN: EPOCH 0013 / 0100 | BATCH 0134 / 0135 | LOSS 0.6810\n",
      "TRAIN: EPOCH 0014 / 0100 | BATCH 0134 / 0135 | LOSS 0.6798\n",
      "TRAIN: EPOCH 0015 / 0100 | BATCH 0134 / 0135 | LOSS 0.6775\n",
      "TRAIN: EPOCH 0016 / 0100 | BATCH 0134 / 0135 | LOSS 0.6777\n",
      "TRAIN: EPOCH 0017 / 0100 | BATCH 0134 / 0135 | LOSS 0.6741\n",
      "TRAIN: EPOCH 0018 / 0100 | BATCH 0134 / 0135 | LOSS 0.6839\n",
      "TRAIN: EPOCH 0019 / 0100 | BATCH 0134 / 0135 | LOSS 0.6771\n",
      "TRAIN: EPOCH 0020 / 0100 | BATCH 0134 / 0135 | LOSS 0.6811\n",
      "TRAIN: EPOCH 0021 / 0100 | BATCH 0134 / 0135 | LOSS 0.6833\n",
      "TRAIN: EPOCH 0022 / 0100 | BATCH 0134 / 0135 | LOSS 0.6816\n",
      "TRAIN: EPOCH 0023 / 0100 | BATCH 0134 / 0135 | LOSS 0.6757\n",
      "TRAIN: EPOCH 0024 / 0100 | BATCH 0134 / 0135 | LOSS 0.6842\n",
      "TRAIN: EPOCH 0025 / 0100 | BATCH 0134 / 0135 | LOSS 0.6786\n",
      "TRAIN: EPOCH 0026 / 0100 | BATCH 0134 / 0135 | LOSS 0.6746\n",
      "TRAIN: EPOCH 0027 / 0100 | BATCH 0134 / 0135 | LOSS 0.6811\n",
      "TRAIN: EPOCH 0028 / 0100 | BATCH 0134 / 0135 | LOSS 0.6790\n",
      "TRAIN: EPOCH 0029 / 0100 | BATCH 0134 / 0135 | LOSS 0.6836\n",
      "TRAIN: EPOCH 0030 / 0100 | BATCH 0134 / 0135 | LOSS 0.6761\n",
      "TRAIN: EPOCH 0031 / 0100 | BATCH 0134 / 0135 | LOSS 0.6835\n",
      "TRAIN: EPOCH 0032 / 0100 | BATCH 0134 / 0135 | LOSS 0.6809\n",
      "TRAIN: EPOCH 0033 / 0100 | BATCH 0134 / 0135 | LOSS 0.6763\n",
      "TRAIN: EPOCH 0034 / 0100 | BATCH 0134 / 0135 | LOSS 0.6821\n",
      "TRAIN: EPOCH 0035 / 0100 | BATCH 0134 / 0135 | LOSS 0.6768\n",
      "TRAIN: EPOCH 0036 / 0100 | BATCH 0134 / 0135 | LOSS 0.6800\n",
      "TRAIN: EPOCH 0037 / 0100 | BATCH 0134 / 0135 | LOSS 0.6786\n",
      "TRAIN: EPOCH 0038 / 0100 | BATCH 0134 / 0135 | LOSS 0.6808\n",
      "TRAIN: EPOCH 0039 / 0100 | BATCH 0134 / 0135 | LOSS 0.6757\n",
      "TRAIN: EPOCH 0040 / 0100 | BATCH 0134 / 0135 | LOSS 0.6806\n",
      "TRAIN: EPOCH 0041 / 0100 | BATCH 0134 / 0135 | LOSS 0.6817\n",
      "TRAIN: EPOCH 0042 / 0100 | BATCH 0134 / 0135 | LOSS 0.6820\n",
      "TRAIN: EPOCH 0043 / 0100 | BATCH 0134 / 0135 | LOSS 0.6761\n",
      "TRAIN: EPOCH 0044 / 0100 | BATCH 0134 / 0135 | LOSS 0.6738\n",
      "TRAIN: EPOCH 0045 / 0100 | BATCH 0134 / 0135 | LOSS 0.6787\n",
      "TRAIN: EPOCH 0046 / 0100 | BATCH 0134 / 0135 | LOSS 0.6801\n",
      "TRAIN: EPOCH 0047 / 0100 | BATCH 0134 / 0135 | LOSS 0.6821\n",
      "TRAIN: EPOCH 0048 / 0100 | BATCH 0134 / 0135 | LOSS 0.6727\n",
      "TRAIN: EPOCH 0049 / 0100 | BATCH 0134 / 0135 | LOSS 0.6776\n",
      "TRAIN: EPOCH 0050 / 0100 | BATCH 0134 / 0135 | LOSS 0.6762\n",
      "TRAIN: EPOCH 0051 / 0100 | BATCH 0134 / 0135 | LOSS 0.6820\n",
      "TRAIN: EPOCH 0052 / 0100 | BATCH 0134 / 0135 | LOSS 0.6724\n",
      "TRAIN: EPOCH 0053 / 0100 | BATCH 0134 / 0135 | LOSS 0.6768\n",
      "TRAIN: EPOCH 0054 / 0100 | BATCH 0134 / 0135 | LOSS 0.6727\n",
      "TRAIN: EPOCH 0055 / 0100 | BATCH 0134 / 0135 | LOSS 0.6817\n",
      "TRAIN: EPOCH 0056 / 0100 | BATCH 0134 / 0135 | LOSS 0.6788\n",
      "TRAIN: EPOCH 0057 / 0100 | BATCH 0134 / 0135 | LOSS 0.6774\n",
      "TRAIN: EPOCH 0058 / 0100 | BATCH 0134 / 0135 | LOSS 0.6763\n",
      "TRAIN: EPOCH 0059 / 0100 | BATCH 0134 / 0135 | LOSS 0.6798\n",
      "TRAIN: EPOCH 0060 / 0100 | BATCH 0134 / 0135 | LOSS 0.6838\n",
      "TRAIN: EPOCH 0061 / 0100 | BATCH 0134 / 0135 | LOSS 0.6787\n",
      "TRAIN: EPOCH 0062 / 0100 | BATCH 0134 / 0135 | LOSS 0.6799\n",
      "TRAIN: EPOCH 0063 / 0100 | BATCH 0134 / 0135 | LOSS 0.6790\n",
      "TRAIN: EPOCH 0064 / 0100 | BATCH 0134 / 0135 | LOSS 0.6721\n",
      "TRAIN: EPOCH 0065 / 0100 | BATCH 0134 / 0135 | LOSS 0.6761\n",
      "TRAIN: EPOCH 0066 / 0100 | BATCH 0134 / 0135 | LOSS 0.6770\n",
      "TRAIN: EPOCH 0067 / 0100 | BATCH 0134 / 0135 | LOSS 0.6804\n",
      "TRAIN: EPOCH 0068 / 0100 | BATCH 0134 / 0135 | LOSS 0.6782\n",
      "TRAIN: EPOCH 0069 / 0100 | BATCH 0134 / 0135 | LOSS 0.6791\n",
      "TRAIN: EPOCH 0070 / 0100 | BATCH 0134 / 0135 | LOSS 0.6857\n",
      "TRAIN: EPOCH 0071 / 0100 | BATCH 0134 / 0135 | LOSS 0.6809\n",
      "TRAIN: EPOCH 0072 / 0100 | BATCH 0134 / 0135 | LOSS 0.6775\n",
      "TRAIN: EPOCH 0073 / 0100 | BATCH 0134 / 0135 | LOSS 0.6820\n",
      "TRAIN: EPOCH 0074 / 0100 | BATCH 0134 / 0135 | LOSS 0.6818\n",
      "TRAIN: EPOCH 0075 / 0100 | BATCH 0134 / 0135 | LOSS 0.6798\n",
      "TRAIN: EPOCH 0076 / 0100 | BATCH 0134 / 0135 | LOSS 0.6816\n",
      "TRAIN: EPOCH 0077 / 0100 | BATCH 0134 / 0135 | LOSS 0.6777\n",
      "TRAIN: EPOCH 0078 / 0100 | BATCH 0134 / 0135 | LOSS 0.6771\n",
      "TRAIN: EPOCH 0079 / 0100 | BATCH 0134 / 0135 | LOSS 0.6821\n",
      "TRAIN: EPOCH 0080 / 0100 | BATCH 0134 / 0135 | LOSS 0.6801\n",
      "TRAIN: EPOCH 0081 / 0100 | BATCH 0134 / 0135 | LOSS 0.6868\n",
      "TRAIN: EPOCH 0082 / 0100 | BATCH 0134 / 0135 | LOSS 0.6782\n",
      "TRAIN: EPOCH 0083 / 0100 | BATCH 0134 / 0135 | LOSS 0.6788\n",
      "TRAIN: EPOCH 0084 / 0100 | BATCH 0134 / 0135 | LOSS 0.6839\n",
      "TRAIN: EPOCH 0085 / 0100 | BATCH 0134 / 0135 | LOSS 0.6845\n",
      "TRAIN: EPOCH 0086 / 0100 | BATCH 0134 / 0135 | LOSS 0.6802\n",
      "TRAIN: EPOCH 0087 / 0100 | BATCH 0134 / 0135 | LOSS 0.6804\n",
      "TRAIN: EPOCH 0088 / 0100 | BATCH 0134 / 0135 | LOSS 0.6818\n",
      "TRAIN: EPOCH 0089 / 0100 | BATCH 0134 / 0135 | LOSS 0.6806\n",
      "TRAIN: EPOCH 0090 / 0100 | BATCH 0134 / 0135 | LOSS 0.6819\n",
      "TRAIN: EPOCH 0091 / 0100 | BATCH 0134 / 0135 | LOSS 0.6796\n",
      "TRAIN: EPOCH 0092 / 0100 | BATCH 0134 / 0135 | LOSS 0.6763\n",
      "TRAIN: EPOCH 0093 / 0100 | BATCH 0134 / 0135 | LOSS 0.6783\n",
      "TRAIN: EPOCH 0094 / 0100 | BATCH 0134 / 0135 | LOSS 0.6751\n",
      "TRAIN: EPOCH 0095 / 0100 | BATCH 0134 / 0135 | LOSS 0.6782\n",
      "TRAIN: EPOCH 0096 / 0100 | BATCH 0134 / 0135 | LOSS 0.6787\n",
      "TRAIN: EPOCH 0097 / 0100 | BATCH 0134 / 0135 | LOSS 0.6837\n",
      "TRAIN: EPOCH 0098 / 0100 | BATCH 0134 / 0135 | LOSS 0.6783\n",
      "TRAIN: EPOCH 0099 / 0100 | BATCH 0134 / 0135 | LOSS 0.6842\n",
      "Finish!!\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "loss_arr = []\n",
    "if mode == 'train':\n",
    "    for epoch in range(0 , 100):\n",
    "        resnet50.train()\n",
    "        loss_arr = []\n",
    "        for batch, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data # input data, label 분리\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = resnet50(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_arr += [loss.item()]\n",
    "            \n",
    "       # writer.add_scalar('trainin loss', np.mean(loss_arr), epoch * len(train_loader)+ batch)\n",
    "        #writer.add_figure('predictions vs. actuals', plot_classes_preds(resnet50, inputs, labels),\n",
    "                         #gloabal_step = epoch * len(train_loader) + batch)\n",
    "        print(\"TRAIN: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f\" %\n",
    "                  (epoch, 100, batch, 135, np.mean(loss_arr)))   \n",
    "        loss_arr = []\n",
    "print('Finish!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "366dc368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set : Average loss:0.7168, Accuracy: 1/4(25%)\n",
      "\n",
      "\n",
      "Test set : Average loss:1.3557, Accuracy: 3/8(38%)\n",
      "\n",
      "\n",
      "Test set : Average loss:2.0192, Accuracy: 5/12(42%)\n",
      "\n",
      "\n",
      "Test set : Average loss:2.7754, Accuracy: 6/16(38%)\n",
      "\n",
      "\n",
      "Test set : Average loss:3.4275, Accuracy: 8/20(40%)\n",
      "\n",
      "\n",
      "Test set : Average loss:4.0885, Accuracy: 10/24(42%)\n",
      "\n",
      "\n",
      "Test set : Average loss:4.7204, Accuracy: 12/28(43%)\n",
      "\n",
      "\n",
      "Test set : Average loss:5.4345, Accuracy: 13/32(41%)\n",
      "\n",
      "\n",
      "Test set : Average loss:6.1056, Accuracy: 15/36(42%)\n",
      "\n",
      "\n",
      "Test set : Average loss:6.6886, Accuracy: 18/40(45%)\n",
      "\n",
      "\n",
      "Test set : Average loss:7.3275, Accuracy: 21/44(48%)\n",
      "\n",
      "\n",
      "Test set : Average loss:8.0454, Accuracy: 23/48(48%)\n",
      "\n",
      "\n",
      "Test set : Average loss:8.8742, Accuracy: 24/52(46%)\n",
      "\n",
      "\n",
      "Test set : Average loss:9.6268, Accuracy: 26/56(46%)\n",
      "\n",
      "\n",
      "Test set : Average loss:10.3377, Accuracy: 28/60(47%)\n",
      "\n",
      "\n",
      "Test set : Average loss:11.1030, Accuracy: 28/64(44%)\n",
      "\n",
      "\n",
      "Test set : Average loss:11.7605, Accuracy: 30/68(44%)\n",
      "\n",
      "\n",
      "Test set : Average loss:12.4134, Accuracy: 32/72(44%)\n",
      "\n",
      "\n",
      "Test set : Average loss:12.9947, Accuracy: 36/76(47%)\n",
      "\n",
      "\n",
      "Test set : Average loss:13.6112, Accuracy: 39/80(49%)\n",
      "\n",
      "\n",
      "Test set : Average loss:14.2528, Accuracy: 41/84(49%)\n",
      "\n",
      "\n",
      "Test set : Average loss:14.8675, Accuracy: 44/88(50%)\n",
      "\n",
      "\n",
      "Test set : Average loss:15.5887, Accuracy: 45/92(49%)\n",
      "\n",
      "\n",
      "Test set : Average loss:16.2399, Accuracy: 47/96(49%)\n",
      "\n",
      "\n",
      "Test set : Average loss:16.9791, Accuracy: 49/100(49%)\n",
      "\n",
      "\n",
      "Test set : Average loss:17.7032, Accuracy: 51/104(49%)\n",
      "\n",
      "\n",
      "Test set : Average loss:18.2791, Accuracy: 55/108(51%)\n",
      "\n",
      "\n",
      "Test set : Average loss:18.8842, Accuracy: 59/112(53%)\n",
      "\n",
      "\n",
      "Test set : Average loss:19.5956, Accuracy: 61/116(53%)\n",
      "\n",
      "\n",
      "Test set : Average loss:20.2324, Accuracy: 64/120(53%)\n",
      "\n",
      "\n",
      "Test set : Average loss:20.9946, Accuracy: 64/124(52%)\n",
      "\n",
      "\n",
      "Test set : Average loss:21.5684, Accuracy: 68/128(53%)\n",
      "\n",
      "\n",
      "Test set : Average loss:22.2443, Accuracy: 70/132(53%)\n",
      "\n",
      "\n",
      "Test set : Average loss:22.9639, Accuracy: 71/135(53%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resnet50.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0\n",
    "acc = []\n",
    "criterion= nn.CrossEntropyLoss().to(device)\n",
    "for batch, data in enumerate(test_loader, 0):\n",
    "    inputs, labels = data # input data, label 분리\n",
    "    inputs = inputs.to(device).float()\n",
    "    labels = labels.to(device)\n",
    "    output = resnet50(inputs)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    test_loss += criterion(output, labels).item()\n",
    "    acc.append(100 * correct/total)\n",
    "\n",
    "    print('\\nTest set : Average loss:{:.4f}, Accuracy: {}/{}({:.0f}%)\\n'.format(\n",
    "                      test_loss, correct, total, 100 * correct/total\n",
    "                  ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
